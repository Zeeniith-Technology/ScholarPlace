[
    {
        "question_id": "W1_C1",
        "week": 1,
        "test_id": "WT001",
        "position": "Capstone_1",
        "question_number": 51,
        "question_text": "You need to generate a pyramid pattern using an array where row i contains the first (i+1) odd numbers. For example, row 0: [1], row 1: [1,3], row 2: [1,3,5]. What is the time complexity to generate this pattern for n rows, and which data structure approach is most memory-efficient?",
        "problem_context": "A developer needs to optimize a pattern generation algorithm that creates numerical pyramids stored in memory for a visualization tool.",
        "concepts_involved": [
            "Nested Loops",
            "Arrays",
            "Pattern Logic",
            "Time Complexity"
        ],
        "option_a": "O(n) time using a 1D array, most memory-efficient",
        "option_b": "O(n²) time using a 2D array, stores all values for easy access",
        "option_c": "O(n log n) time using dynamic array resizing",
        "option_d": "O(n²) time but generate on-the-fly without storage for best memory efficiency",
        "correct_answer": "D",
        "explanation": {
            "why_correct": "Generating n rows requires summing 1+2+3+...+n = n(n+1)/2 iterations, which is O(n²). Option D is correct because while time complexity is O(n²), generating values on-the-fly without storing them uses O(1) space instead of O(n²) for a 2D array, making it most memory-efficient.",
            "concept_analysis": "This combines loop iteration counting (nested loops create quadratic time), pattern formulas (odd numbers: 2k+1), and memory tradeoffs. Students must recognize that row i has (i+1) elements, totaling n(n+1)/2 elements across all rows.",
            "time_complexity": "O(n²) - outer loop runs n times, inner loop runs 1+2+3+...+n times total",
            "space_complexity": "O(1) if generated on-the-fly, O(n²) if stored in 2D array",
            "common_mistakes": "Thinking outer loop of n iterations means O(n) time, forgetting that inner loops grow linearly creating quadratic growth",
            "optimization_tip": "If values are only displayed once, generate on-the-fly. If accessed repeatedly, store in 2D array despite memory cost.",
            "edge_cases": "n=0 (no rows), n=1 (single element), very large n causing memory overflow if stored"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 240,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W1_C2",
        "week": 1,
        "test_id": "WT001",
        "position": "Capstone_2",
        "question_number": 52,
        "question_text": "A function processArray(arr, size) finds the maximum element by iterating through the array. The function is called with size = arr.length + 5 due to a bug. What is the safest way to prevent array out-of-bounds access while maintaining O(n) time complexity?",
        "problem_context": "Production code has a bug where array size is incorrectly passed to a processing function, causing crashes in edge cases.",
        "concepts_involved": [
            "Arrays",
            "Functions",
            "Boundary Checking",
            "Error Handling"
        ],
        "option_a": "Add if (i < size) check inside the loop for every iteration",
        "option_b": "Before the loop, set size = min(size, actual_array_length) to clamp the size",
        "option_c": "Use try-catch inside the loop to handle out-of-bounds exceptions",
        "option_d": "Resize the array to match the provided size by padding with dummy values",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "Option B fixes the issue once before the loop by clamping size to the actual array length. This is O(1) preprocessing that prevents all out-of-bounds access while maintaining O(n) loop complexity. It's defensive programming at function entry.",
            "concept_analysis": "This tests understanding of function parameter validation, array bounds safety, and complexity preservation. The key insight is to validate/fix parameters at function entry rather than repeatedly checking inside loops.",
            "time_complexity": "O(n) - single pass through valid array length, O(1) preprocessing",
            "space_complexity": "O(1) - no additional space needed",
            "common_mistakes": "Option A adds unnecessary O(n) bounds checks inside the loop when one check before the loop suffices. Option C using exceptions is inefficient and poor practice. Option D wastes memory and changes array semantics.",
            "optimization_tip": "Always validate function parameters at entry point. Use min/max clamping for size parameters.",
            "edge_cases": "size = 0, size < 0, size >> array length, null array"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 210,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W2_C1",
        "week": 2,
        "test_id": "WT002",
        "position": "Capstone_1",
        "question_number": 51,
        "question_text": "Given a sorted array of n integers and a target sum, you need to find if any two distinct elements sum to the target. Compare three approaches: (1) Brute force nested loops O(n²), (2) Binary search for each element O(n log n), (3) Two-pointer from ends O(n). Which approach is optimal for large n, and why?",
        "problem_context": "A financial application needs to find if any two transaction amounts sum to a suspicious total, processing millions of records daily.",
        "concepts_involved": [
            "Binary Search",
            "Two-Pointer",
            "Time Complexity",
            "Optimization"
        ],
        "option_a": "Binary search O(n log n) because it uses the sorted property efficiently",
        "option_b": "Two-pointer O(n) because it's linear time and uses the sorted property optimally",
        "option_c": "Brute force O(n²) because it's simpler to implement and has O(1) space",
        "option_d": "Binary search O(n log n) because two-pointer doesn't work on sorted arrays",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "Two-pointer is optimal: O(n) time, O(1) space. Starting from both ends of the sorted array, move pointers based on sum comparison. If arr[left] + arr[right] > target, decrement right; if sum < target, increment left. This exploits sorting to eliminate half the search space at each step without extra data structures.",
            "concept_analysis": "This tests understanding of algorithmic optimization and recognizing when linear algorithms can replace logarithmic ones by exploiting data properties (sorted order). Binary search for each element is O(n log n) which is slower than O(n) two-pointer for large n.",
            "time_complexity": "Two-pointer: O(n), Binary search: O(n log n), Brute force: O(n²)",
            "space_complexity": "All approaches use O(1) space (no extra arrays needed)",
            "common_mistakes": "Thinking binary search is always faster than linear approaches, not recognizing that two-pointer can achieve O(n) on sorted arrays, choosing brute force for simplicity despite massive performance hit",
            "optimization_tip": "On sorted data, two-pointer technique often achieves O(n) for problems that would otherwise require O(n log n) or O(n²)",
            "edge_cases": "Array with duplicates, target sum = 2 * element (need distinct elements), empty array, single element"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 270,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W2_C2",
        "week": 2,
        "test_id": "WT002",
        "position": "Capstone_2",
        "question_number": 52,
        "question_text": "You're parsing a CSV string like 'Alice,25,Engineer' into an array of fields. Which edge cases are critical to handle for robust parsing?",
        "problem_context": "A data import system processes user-uploaded CSV files that may contain malformed data, causing production crashes.",
        "concepts_involved": [
            "String Parsing",
            "Arrays",
            "Edge Cases",
            "Input Validation"
        ],
        "option_a": "Only handle empty strings and null inputs",
        "option_b": "Handle: empty string, leading/trailing spaces, consecutive delimiters (empty fields), missing fields, delimiter inside quoted fields",
        "option_c": "Just split by comma and trim spaces, no special cases needed",
        "option_d": "Handle only consecutive delimiters (,,) creating empty fields",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "Option B covers all critical edge cases: (1) Empty string → empty array, (2) Leading/trailing spaces → trim, (3) Consecutive delimiters 'a,,c' → preserve empty field, (4) Fewer fields than expected → handle gracefully, (5) Quotes 'John,\"Smith, Jr\",25' → delimiter inside quotes shouldn't split. Robust CSV parsing requires handling all these cases.",
            "concept_analysis": "This tests comprehensive understanding of string parsing complexity and real-world input validation. CSV parsing is deceptively complex due to edge cases that appear in production data.",
            "time_complexity": "O(n) where n is string length - single pass through string",
            "space_complexity": "O(n) for storing parsed fields in array",
            "common_mistakes": "Using simple split(',') without handling quoted fields or empty fields, forgetting to trim spaces, not validating field count, ignoring empty input",
            "optimization_tip": "Use state machine approach to handle quotes and delimiters correctly in one pass",
            "edge_cases": "Empty string, single field, trailing comma, quoted delimiters, escaped quotes, null input, very long fields"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 240,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W3_C1",
        "week": 3,
        "test_id": "WT003",
        "position": "Capstone_1",
        "question_number": 51,
        "question_text": "To determine if a string can be rearranged to form a palindrome, you count character frequencies. What property must the frequency distribution satisfy for a palindrome to be possible?",
        "problem_context": "A word game application checks if shuffled letters can form a palindrome before allowing a move.",
        "concepts_involved": [
            "String Manipulation",
            "Frequency Counting",
            "Palindrome Logic",
            "Mathematical Reasoning"
        ],
        "option_a": "All characters must have even frequency",
        "option_b": "At most one character can have odd frequency (for the middle position)",
        "option_c": "Exactly half the characters must have even frequency",
        "option_d": "The most frequent character must appear at least twice",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "For a palindrome, characters must mirror around the center. In 'racecar', all characters except 'e' appear an even number of times (r:2, a:2, c:2), and 'e' appears once (odd) in the middle. For even-length palindromes like 'abba', all characters have even frequency. Thus, at most ONE character can have odd frequency (used in the middle for odd-length palindromes).",
            "concept_analysis": "This combines string properties, frequency analysis, and palindrome structure understanding. Students must recognize that palindromes are symmetric, requiring paired characters except for one optional middle character.",
            "time_complexity": "O(n) to count frequencies using hash map/array",
            "space_complexity": "O(1) for fixed alphabet size (e.g., 26 lowercase letters) or O(k) for k unique characters",
            "common_mistakes": "Thinking all frequencies must be even (fails for odd-length palindromes), not understanding the middle character exception, confusing 'can be rearranged' with 'is currently a palindrome'",
            "optimization_tip": "Use array of size 26 for lowercase English letters instead of hash map for O(1) space and faster access",
            "edge_cases": "Empty string (valid palindrome), single character (valid), all unique characters (only 1-char strings can form palindrome), case sensitivity"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 255,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W3_C2",
        "week": 3,
        "test_id": "WT003",
        "position": "Capstone_2",
        "question_number": 52,
        "question_text": "Convert a space-separated string 'apple banana cherry' into a linked list where each word is a node. What is the correct parsing and pointer management approach?",
        "problem_context": "A text editor represents undo history as a linked list of words for efficient insertion and deletion.",
        "concepts_involved": [
            "String Parsing",
            "Linked Lists",
            "Pointer Management",
            "Dynamic Allocation"
        ],
        "option_a": "Split string into array first, then convert array to linked list in two passes",
        "option_b": "Parse string character-by-character, create node when space is encountered, maintain head and tail pointers for O(n) single-pass insertion",
        "option_c": "Use recursion to parse each word and create nodes",
        "option_d": "Create all nodes first with empty data, then fill them by parsing",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "Option B is optimal: single-pass O(n) parsing with O(1) node insertion. Maintain head pointer (first node) and tail pointer (last node). While parsing, build current word; when space is encountered, create node with current word, append to tail, update tail. This avoids the O(n) extra space of array intermediate storage and processes in one pass.",
            "concept_analysis": "This tests string parsing integrated with linked list construction, understanding pointer management (head/tail pattern), and single-pass optimization. Students must recognize that tail pointer enables O(1) append instead of O(n) traversal for each insertion.",
            "time_complexity": "O(n) for single pass through string, O(1) per node insertion with tail pointer",
            "space_complexity": "O(m) for m words stored in linked list nodes",
            "common_mistakes": "Creating array first (unnecessary extra space), not maintaining tail pointer (causing O(n²) for repeated traversal to append), forgetting to handle last word (no trailing space), memory leak if nodes aren't properly linked",
            "optimization_tip": "Always maintain tail pointer when building linked list to avoid O(n) traversal for each append",
            "edge_cases": "Empty string, single word (no spaces), leading/trailing spaces, multiple consecutive spaces, very long words"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 270,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W4_C1",
        "week": 4,
        "test_id": "WT004",
        "position": "Capstone_1",
        "question_number": 51,
        "question_text": "If a linked list has a cycle, can you safely reverse the entire list? What happens if you attempt reversal using the standard iterative 3-pointer approach (prev, curr, next)?",
        "problem_context": "A list processing system needs to reverse linked lists, but some lists may be corrupted with cycles from improper deletion operations.",
        "concepts_involved": [
            "Linked List Reversal",
            "Cycle Detection",
            "Pointer Logic",
            "Algorithm Safety"
        ],
        "option_a": "Yes, reversal works fine; the cycle just reverses direction",
        "option_b": "No, reversal on a cyclic list will cause infinite loop because curr never becomes NULL",
        "option_c": "Yes, but only if you detect the cycle first using Floyd's algorithm and break it",
        "option_d": "Reversal works but creates two separate cycles",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "Standard iterative reversal uses: while(curr != NULL) { next = curr->next; curr->next = prev; prev = curr; curr = next; }. This terminates when curr reaches NULL (end of list). In a cyclic list, curr NEVER becomes NULL - it cycles infinitely. Thus, reversal enters infinite loop without terminating. Option C is partially correct but doesn't directly answer what happens if you attempt reversal without detection.",
            "concept_analysis": "This tests understanding of both reversal algorithm mechanics and cycle implications. Students must recognize that reversal's termination condition (curr == NULL) is never satisfied in cyclic lists, and understand why Floyd's algorithm detects cycles using different pointer speeds (fast pointer eventually laps slow pointer).",
            "time_complexity": "Infinite (non-terminating) on cyclic list, O(n) on acyclic list",
            "space_complexity": "O(1) for iterative reversal",
            "common_mistakes": "Thinking reversal just changes cycle direction (it doesn't terminate), not recognizing NULL termination requirement, confusing cycle detection with cycle breaking",
            "optimization_tip": "Always run cycle detection (Floyd's) before reversal. If cycle exists, break it first by setting cycle_end->next = NULL after finding cycle start",
            "edge_cases": "Self-loop (single node pointing to itself), cycle at any position (start, middle, end), entire list is a cycle (no tail)"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 285,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W4_C2",
        "week": 4,
        "test_id": "WT004",
        "position": "Capstone_2",
        "question_number": 52,
        "question_text": "Merging two sorted linked lists L1 (length m) and L2 (length n) can be done by: (1) Creating a new merged list, or (2) Rearranging existing nodes in-place. Compare the time and space complexity of both approaches. Which is better?",
        "problem_context": "A database system merges sorted query result lists from different nodes, processing billions of records where memory efficiency is critical.",
        "concepts_involved": [
            "Linked List Merging",
            "Pointer Manipulation",
            "Time-Space Complexity",
            "In-place Algorithms"
        ],
        "option_a": "New list: O(m+n) time, O(m+n) space; In-place: O(m+n) time, O(1) space. In-place is better.",
        "option_b": "New list: O(m+n) time, O(1) space; In-place: O(m+n) time, O(m+n) space. New list is better.",
        "option_c": "Both have O(m+n) time and O(m+n) space; no difference",
        "option_d": "New list: O((m+n) log(m+n)) time for sorting; In-place: O(m+n) time. In-place is better.",
        "correct_answer": "A",
        "explanation": {
            "why_correct": "Both approaches visit each node once: O(m+n) time. NEW LIST: Allocates new nodes for merged list: O(m+n) extra space. IN-PLACE: Rearranges existing nodes by changing next pointers (no new nodes): O(1) extra space. For large lists (m, n in billions), O(m+n) space is massive memory overhead. In-place is superior for memory efficiency with same time complexity. Use dummy head node technique to simplify in-place merging.",
            "concept_analysis": "This tests understanding of space complexity, in-place algorithm benefits, and linked list pointer manipulation. Students must recognize that creating new nodes vs rearranging pointers has same time cost but vastly different space cost.",
            "time_complexity": "Both O(m+n) - must visit each node once to compare and merge",
            "space_complexity": "New list: O(m+n) for new nodes; In-place: O(1) auxiliary space (only dummy node and pointers)",
            "common_mistakes": "Thinking new list is faster (same time), not recognizing in-place possibility for linked lists (unlike arrays), confusing recursion stack space with data structure space, thinking in-place is O(m+n) space",
            "optimization_tip": "Use dummy head node to avoid special cases for head pointer in in-place merge. Always prefer in-place for linked lists when possible to save memory.",
            "edge_cases": "One list empty, lists of very different lengths, all elements in L1 < all in L2, duplicate values across lists"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 270,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W5_C1",
        "week": 5,
        "test_id": "WT005",
        "position": "Capstone_1",
        "question_number": 51,
        "question_text": "You're designing an undo/redo system for a text editor. Users can undo operations (Ctrl+Z) and redo them (Ctrl+Y). Which data structure(s) should you use and why?",
        "problem_context": "A document editor must support unlimited undo/redo for all text operations with instant response time.",
        "concepts_involved": [
            "Stack Operations",
            "LIFO Logic",
            "Application Design",
            "Data Structure Selection"
        ],
        "option_a": "Single queue (FIFO) to store all operations in order",
        "option_b": "Two stacks: undo_stack (LIFO) for past operations, redo_stack (LIFO) for undone operations",
        "option_c": "Array with index pointer to track current position",
        "option_d": "Single stack that reverses when switching between undo/redo",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "Two stacks is optimal: UNDO_STACK stores operations in reverse order (most recent on top). On Ctrl+Z, pop from undo_stack, reverse operation, push to redo_stack. On Ctrl+Y, pop from redo_stack, execute it, push to undo_stack. This exploits LIFO: most recent operation is always accessible in O(1). When user performs new operation after undo, clear redo_stack (redo history invalidated). Both undo and redo are O(1).",
            "concept_analysis": "This tests understanding of stack LIFO property for practical application. Undo needs most-recent-first access (LIFO), not oldest-first (FIFO). Arrays work but stacks are conceptually clearer and don't need size preallocation. Students must recognize the two-stack pattern for bidirectional history.",
            "time_complexity": "O(1) for both undo and redo operations using stacks",
            "space_complexity": "O(n) where n is number of operations stored in history",
            "common_mistakes": "Using queue (FIFO gives wrong order), using single stack (complex reversal logic), using array without understanding it's essentially a stack implementation, not clearing redo_stack on new operation",
            "optimization_tip": "Limit stack sizes to prevent unbounded memory growth (e.g., max 1000 undo levels). Use command pattern to encapsulate operations.",
            "edge_cases": "Undo when no more operations (empty stack), redo when redo_stack empty, new operation after partial undo (invalidates redo), memory limits for large documents"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 255,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    },
    {
        "question_id": "W5_C2",
        "week": 5,
        "test_id": "WT005",
        "position": "Capstone_2",
        "question_number": 52,
        "question_text": "A customer service system processes support tickets. Tickets should be handled in the order they arrive (first-come, first-served fairness). Would you use a stack (LIFO) or queue (FIFO)? What happens if you mistakenly use the wrong data structure?",
        "problem_context": "A help desk application must ensure fair handling of customer requests to maintain service quality and customer satisfaction.",
        "concepts_involved": [
            "Queue Operations",
            "FIFO Logic",
            "Fairness",
            "Stack vs Queue"
        ],
        "option_a": "Use stack (LIFO) because it's faster; fairness isn't critical",
        "option_b": "Use queue (FIFO) because first-come, first-served requires processing in arrival order; stack would process newest first, starving old tickets",
        "option_c": "Both stack and queue work equally well for this use case",
        "option_d": "Use queue but enqueue and dequeue from same end to simulate stack for efficiency",
        "correct_answer": "B",
        "explanation": {
            "why_correct": "Queue MUST be used for fairness. FIFO ensures ticket submitted at 9am is processed before 9:01am ticket. If using stack (LIFO), newest tickets are always processed first, causing 'starvation' - old tickets never get processed if new tickets keep arriving. This violates fairness and service quality. Real-world impact: customer who submitted ticket hours ago sees newer tickets resolved first, causing frustration and complaints.",
            "concept_analysis": "This tests understanding of FIFO vs LIFO practical implications and data structure selection based on requirements. Students must recognize that the order of processing matters for fairness, and that stack's LIFO fundamentally conflicts with first-come-first-served requirements.",
            "time_complexity": "O(1) for both enqueue and dequeue operations in queue",
            "space_complexity": "O(n) for n pending tickets in queue",
            "common_mistakes": "Thinking stack and queue are interchangeable, not understanding starvation problem, valuing implementation simplicity over correctness, not considering real-world fairness implications",
            "optimization_tip": "Implement priority queue to handle urgent tickets while maintaining FIFO for same priority level. Use circular queue array implementation for memory efficiency.",
            "edge_cases": "Queue overflow if too many tickets, empty queue (no tickets), priority conflicts (urgent vs FIFO), concurrent access in multi-threaded system"
        },
        "difficulty": "MEDIUM",
        "difficulty_rating": 4,
        "estimated_time_seconds": 240,
        "logic_type": "COMBINED_REASONING",
        "requires_coding": false,
        "is_based_on_bank": false,
        "repetition_check": "UNIQUE - NOT IN JSON"
    }
]